{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelos y sus métricas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Clasificación (Predecir una clase - binaria / multinomial)\n",
    "  - **Matriz de confusión:** Evalúa cómo un modelo clasifica correctamente e incorrectamente las instancias según las predicciones y los valores reales.\n",
    "  - **Exactitud (Accuracy):** Proporción de predicciones correctas sobre el total de muestras.\n",
    "  - **Precisión:** Proporción de instancias predichas como positivas que son realmente positivas.\n",
    "  - **Sensibilidad (Recall):** Proporción de instancias positivas que fueron correctamente predichas por el modelo.\n",
    "  - **Media Armónica Precisión-Sensibilidad (F1 Score):** Combina precisión y recall en una medida única, útil para evaluar la precisión general del modelo.\n",
    "\n",
    "### B. Regresión (Predecir un número cuantitativo - entero / continuo)\n",
    "  - **Error Absoluto Medio (MAE):** Media de las diferencias absolutas entre las predicciones y los valores reales.\n",
    "  - **Error Cuadrático Medio (MSE):** Media de las diferencias al cuadrado entre las predicciones y los valores reales.\n",
    "  - **Raíz del Error Cuadrático Medio (RMSE):** Raíz cuadrada del MSE, proporciona una métrica en la misma escala que los valores originales.\n",
    "  - **Coeficiente de Determinación (R2):** Indica qué tan bien se ajustan las predicciones del modelo a los valores observados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Métricas de Clasificación (Binario / Multinomial)**\n",
    "\n",
    "#### **Matriz de Confusión**\n",
    "\n",
    "La matriz de confusión es una herramienta fundamental para evaluar el rendimiento de un modelo de clasificación al comparar las predicciones con los valores reales. Proporciona una visión detallada de cómo el modelo clasifica correctamente e incorrectamente las instancias.\n",
    "\n",
    "|                | Predicción Positiva | Predicción Negativa |\n",
    "|----------------|---------------------|---------------------|\n",
    "| Real Positivo  | True Positive (TP)  | False Negative (FN) |\n",
    "| Real Negativo  | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "- **True Positive (TP):** Instancias que fueron correctamente predichas como positivas.\n",
    "- **False Negative (FN):** Instancias que fueron incorrectamente predichas como negativas.\n",
    "- **False Positive (FP):** Instancias que fueron incorrectamente predichas como positivas.\n",
    "- **True Negative (TN):** Instancias que fueron correctamente predichas como negativas.\n",
    "\n",
    "#### **Exactitud (Accuracy)**\n",
    "\n",
    "La exactitud mide la proporción de predicciones correctas realizadas por el modelo sobre el total de muestras.\n",
    "\n",
    "$ \\text{Accuracy (Acc)} = \\frac{TP + TN}{TP + TN + FP + FN} $\n",
    "\n",
    "Es una medida general de la precisión del modelo, pero puede ser engañosa en conjuntos de datos desequilibrados.\n",
    "\n",
    "#### **Precisión (Precision)**\n",
    "\n",
    "La precisión mide la proporción de instancias predichas como positivas que son realmente positivas.\n",
    "\n",
    "$ \\text{Precision (P)} = \\frac{TP}{TP + FP} $\n",
    "\n",
    "Es útil cuando el coste de los falsos positivos es alto, como en el diagnóstico médico.\n",
    "\n",
    "#### **Sensibilidad (Recall)**\n",
    "\n",
    "La sensibilidad, también conocida como recall o tasa de verdaderos positivos, mide la proporción de instancias positivas que fueron correctamente predichas por el modelo.\n",
    "\n",
    "$ \\text{Recall (R)} = \\frac{TP}{TP + FN} $\n",
    "\n",
    "Es importante en casos donde la detección de positivos es crucial, como en pruebas de enfermedades.\n",
    "\n",
    "#### **Puntuación F1 (F1 Score)**\n",
    "\n",
    "El F1 Score es la media armónica de precisión y recall. Proporciona una medida única que combina ambos aspectos del rendimiento del modelo. Un valor más cercano a 1 indica un modelo más preciso y sensible.\n",
    "\n",
    "$ \\text{F1 Score} = 2 \\cdot \\frac{P \\cdot R}{P + R} $\n",
    "\n",
    "Es especialmente útil cuando hay desigualdad en la distribución de las clases en los datos.\n",
    "\n",
    "Estas métricas proporcionan una evaluación completa del rendimiento de un modelo de clasificación desde diferentes perspectivas: exactitud general, capacidad para predecir correctamente clases específicas y la combinación de precisión y sensibilidad en el F1 Score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Metricas de Regresión (cuantitativo)**\n",
    "\n",
    "#### **MAE**\n",
    "\n",
    "El Error Absoluto Medio (MAE) se define como la media del valor absoluto de las diferencias entre las predicciones y los valores reales.\n",
    "\n",
    "$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $\n",
    "\n",
    "Donde:\n",
    "- $( n )$ es el número total de muestras.\n",
    "- $( y_i )$ es el valor real de la muestra $( i )$.\n",
    "- $( \\hat{y}_i )$ es la predicción para la muestra $( i )$.\n",
    "\n",
    "#### **MSE**\n",
    "\n",
    "El Error Cuadrático Medio (MSE) es otra métrica comúnmente utilizada para evaluar la precisión de un modelo de regresión. Se define como la media de las diferencias al cuadrado entre las predicciones y los valores reales.\n",
    "\n",
    "$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "Donde:\n",
    "- $( n )$ es el número total de muestras.\n",
    "- $( y_i )$ es el valor real de la muestra $( i )$.\n",
    "- $( \\hat{y}_i )$ es la predicción para la muestra $( i )$.\n",
    "\n",
    "#### **RMSE**\n",
    "\n",
    "El Error Cuadrático Medio de la Raíz (RMSE) es una métrica comúnmente utilizada en problemas de regresión para evaluar la precisión de las predicciones. Se calcula como la raíz cuadrada del MSE (Error Cuadrático Medio).\n",
    "\n",
    "$ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $\n",
    "\n",
    "Donde:\n",
    "- $( n )$ es el número total de muestras.\n",
    "- $( y_i )$ es el valor real de la muestra $( i )$.\n",
    "- $( \\hat{y}_i )$ es la predicción para la muestra $( i )$.\n",
    "\n",
    "#### **Coeficiente de determinación $( R^2 )$**\n",
    "\n",
    "El coeficiente de determinación $( R^2 )$ es una medida estadística que indica qué tan bien se ajustan los valores predichos por el modelo a los valores observados. \n",
    "\n",
    "$ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $\n",
    "\n",
    "Donde:\n",
    "- $( n )$ es el número total de muestras.\n",
    "- $( y_i )$ es el valor real de la muestra $( i )$.\n",
    "- $( \\hat{y}_i )$ es la predicción para la muestra $( i )$.\n",
    "- $( \\bar{y} )$ es la media de los valores reales $( y_i )$.\n",
    "\n",
    "#### **Coeficiente de determinación ajustado $( R^2_{\\text{ajustado}} )$**\n",
    "\n",
    "El coeficiente de determinación ajustado $( R^2_{\\text{ajustado}} )$ es una versión corregida del $( R^2 )$ estándar que tiene en cuenta el número de variables explicativas en el modelo.\n",
    "\n",
    "$ R^2_{\\text{ajustado}} = 1 - \\frac{(1 - R^2) \\cdot (n - 1)}{n - p - 1} $\n",
    "\n",
    "Donde:\n",
    "- $( R^2 )$ es el coeficiente de determinación estándar.\n",
    "- $( n )$ es el número total de muestras.\n",
    "- $( p )$ es el número de variables predictoras en el modelo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
